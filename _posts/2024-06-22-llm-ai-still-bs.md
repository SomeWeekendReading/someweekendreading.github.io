---
layout: post
title: AI Large Lanuage Models&colon; Still a BS Firehose
tags: ArtificialIntelligence CatBlogging CorporateLifeAndItsDiscontents JournalClub NotableAndQuotable Sadness SomebodyAskedMe
comments: true
commentsClosed: true
---

[Somebody asked me]({{ site.baseurl}}/tags/#SomebodyAskedMe), in the course of a family
video call, what I thought about the latest AI Large Language Models (LLMs) and how they're being
used for _everything._ Have they improved from their previous BS conditions?  Ahem.
Uh&hellip; _no._  


## The Weekend Prior  

[This post is slightly delayed, by brain fog and by life in general.  So it's being posted
about 9 days after I had intended.  Hence a post date of 2024-Jun-22, but some references
below which are later.]  

We've previously expressed some&hellip; _acerbic_ opinions about LLMs and their various
forms of misbehavior, obfuscation, and generally highly persuasive BS.

For example, their inexhaustible BS fountains of persuasive 
misdirection <sup id="fn1a">[[1]](#fn1)</sup>:  

> What do you call a person who is very good at sounding persuasive and plausible, but
> absolutely bereft of fidelity to fact? A BS artist.  
>  
> &hellip;  
>  
> When I asked it a technical question, it made a very plausible-sounding argument,
> complete with citations to the relevant scientific literature. I was really impressed:
> the papers it cited were by famous scientists working in the correct area, published in
> important journals, with titles that were spot-on relevant to my interests. So why
> hadn’t I, as a scientist familiar with the area, already read those papers?
> __Because they were all fake!__ Every single one was an hallucination, absolutely bereft
> of existence.  

Just in case that's not enough, later in that post we note a reporter's experience: a
Microsoft AI threatened to break up his marriage and murder him!  Were this an actual
person, that would have been an actual _crime._  

The name "ChatGPT" itself, of course, is a rather bizarre name, especially in
French. <sup id="fn2a">[[2]](#fn2)</sup>  "Chat, j'ai p&eacute;t&eacute;" is mildly
transgressive in French, but borderline comically scatological in English.  

Of _course_ people abuse it.  There's more work to be done in fact-checking
_every single word_ than there is in writing yourself in the first place.  That hasn't
stopped people from being na&iuml;vely charmed by the persuasive powers of LLMs:  
- Academics reviewing articles by their colleagues for publication in journals have
  occasionally skimped on the task, trying to sneak in an AI review. <sup id="fn3a">[[3]](#fn3)</sup>
  The results would, of course, be hilarious if they were not real.  
- Even more disgusting, entire papers have been submitted to journals that bear the
  stigmata of having been AI-generated <sup id="fn4a">[[4]](#fn4)</sup>.  They are, of
  course, nonsense, but some have nonetheless sneaked past peer review.  

With _that_ as a Bayesian prior, do we have evidence sufficient to change our minds?  


## Better Now?  Nope, Still BS  

### Bias optimization, and how to react to structural lying  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-futurism-1.jpg" width="400" height="341" alt="Tangerman @ The Byte: Cook says Apple AI may never stop lying" title="Tangerman @ The Byte: Cook says Apple AI may never stop lying" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
It seems every tech company wants to force AI down your throat, putting it into products
whether you want it or not.  For example, Google searches now place an AI guess at your
search at the front of a query (unless you append ['&amp;udm=14'](https://udm14.com/) as a
de-mumble-ification cheat code &ndash; to Bowdlerize the term due to Cory Doctorow).  

Apple, alas, is no different: they announced a suite of AI stuff at WWDC, despite the
problems inherent in that. <sup id="fn5a">[[5]](#fn5)</sup>  The lying is probably an
_intrinsic_ property of LLMs, but they are determined to deploy it anyway:  

>&hellip; Apple CEO Tim Cook admitted outright that he's not entirely sure his tech empire's
> latest "Apple Intelligence" won't come up with lies and confidently distort the truth, a
> problematic and likely intrinsic tendency that has plagued pretty much all AI chatbots
> released to date.  
>  
> __Pants on Fire__  
>  
> It's an uncomfortable reality, especially considering just how laser-focused the tech
> industry and Wall Street have been on developing AI chatbots. Despite tens of billions
> of dollars being poured into the tech, AI tools are repeatedly being caught coming up
> with obvious falsehoods and — perhaps more worryingly — convincingly told lies. 

<a href="https://xkcd.com/1838/"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-xkcd-1.jpg" width="400" height="473" alt="XKCD #1838: 'machine learning' buried in linear algebra meets sarcasm" title="XKCD #1838: 'machine learning' buried in linear algebra meets sarcasm" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>
In other words, matters are as presaged by this as-usual-prophetic
[XKCD #1838](https://xkcd.com/1838/).  "Just stir the pile until the answers start looking
right" is _nothing near_ a strategy for truth-telling!   

Do you just pile up steel beams to make a bridge, and rearrange them until cars no longer
fall off?  Of course not!  You try to _know what you are doing_, in the civil engineering
sense and the social impact sense, _before_ you start building.  

Nothing less than that will do here.  

Also, we should make sure our AIs are, to use a technical term, "aligned" with human
values.  Otherwise, once we make an unaligned Artificial Super Intelligence (ASI),
humanity dies shortly thereafter.  

Summary from a philosophy professor of UNC Charlotte, specializing in science, technology, and society,
reacting to the same information:  
<a href="https://mathstodon.xyz/@Wolven@ourislandgeorgia.net/112622293048460846"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mastodon-1.jpg" width="550" height="500" alt="Williams @ Mastodon: DOES NOT WORK and SHOULD NOT BE USED" title="Williams @ Mastodon: DOES NOT WORK and SHOULD NOT BE USED" style="margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>
<iframe width="400" height="224" src="https://www.youtube.com/embed/9DpM_TXq2ws?si=C9iWEJKfrDevmBIZ" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;"></iframe>
<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-amsci-1.jpg" width="400" height="278" alt="Williams @ American Scientist: They're bias optimizers!" title="Williams @ American Scientist: They're bias optimizers!" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
The bit where he says, "Look at me.  Listen to me:" is important.  Any source that is
highly persuasive, but not committed to fact in anyway, DOES NOT WORK and SHOULD NOT BE
USED.  Excellent advice, that.  

In case you are not persuaded by social media (good for you!), Prof. Williams has provided
both a video of a seminar, shown here, evocatively titled "On Bullsh\*t Engines", and a
more formally peer reviewed article published in _American Scientist_. <sup id="fn6a">[[6]](#fn6)</sup>  

A pointed example from the video (4:03), on AI-generated guides to foraging for edible
mushrooms, available on Amazon despite its many errors:  

> Now, when it comes to misidentifying plants out in the wild, you can have a bad time in
> a number of ways.  One of those ways is, you know, you misidentify something as safe
> when it's actually poisonous, you get a rash.  But when it comes to things like mushroom
> guides and foraging for food one of the ways that that can go wrong is, uh&hellip; __you die.__  

As writer Terry Pratchett is alleged to have said, "All mushrooms are edible, but some
only once."  Idiotic AI systems like this will unerringly guide you to that one last
mushroom.  

He has lots of other examples, some showing horrifying mistakes because the AI was trained
on rather raw Internet texts full of prejudice, racism, sexism, and fascism.  Why in the
world would you want to listen to _that?_  

At 38:55 in the video:  
> When we uncritically make use of these tools what we are doing is we are
> muddying the process of generating knowledge together.  We
> are embodying and empowering a system which __does not in any way shape or form
> care about what is true__ or what is factual, does not care about the impacts &hellip;  
>  
> &hellip;  
> __They do not care about truth. They do not care about fact. They are in fact bullsh\*t
> engines.__  

### Liability Issues  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-atlantic-1.jpg" width="400" height="460" alt="Wong @ Atlantic: Google as a libel machine" title="Wong @ Atlantic: Google as a libel machine" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-wapo-1.jpg" width="400" height="176" alt="Hunter @ WaPo: AI mushroom identifier will kill you" title="Hunter @ WaPo: AI mushroom identifier will kill you" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
It's bad enough that they just lie with impunity: bullsh\*t doesn't so much oppose truth,
as it totally _ignores_ truth as being irrelevant.  It's so bad, people are exposed to 
_lethal risks_, as shown in the mushroom example above.  

But, in accordance with [the 3rd Rule of Crime by writer John Rogers](https://buttondown.email/kungfumonkey/archive/cons-heists-101-orientation/#:~:text=Rule%203%3A%20Nothing%20ever%20stops%20until%20a%20Rich%20White%20Guy%20goes%20to%20jail):  
> __Rule 3:__ Nothing ever stops until a Rich White Guy goes to jail.  

&hellip; it now appears Google's lawyers are becoming &ndash; slowly and dimly &ndash; aware that their
main product is turning into a liability machine.  This is both through generating 
libel <sup id="fn7a">[[7]](#fn7)</sup>, and, of course, the 
mushrooms <sup id="fn8a">[[8]](#fn8)</sup> (which seems to be everybody's favorite example now).  

The current case appears to be an implication by Google that a high-level chess player
admitted to "using an engine", i.e., cheating.  He admitted no such thing, and apparently
no such thing happened.  The first liability suit was for $100 million; though it failed
on procedural grounds the victim with without a doubt take a second shot.  The allegation
of cheating happened repeatedly, until the _Atlantic_ reporter called them for a comment,
at which point it was quickly hidden in typical craven corporate fashion.  

Google's AI has also asserted that one should eat a small rock every day, and that Barak
Obama is Muslim.  This is what happens when you train on the public Internet texts!  

Microsoft's Bing has a similar history of libel: it asserted that veteran aviation
consultant Jeffrey Battle had pled guilty to seditious conspiracy against the United
States, when in fact it was an entirely distinct person with a similar name.  A person
would know to _check the facts_ before saying something like that; AIs currently have no
commitment to the truth in that regard.  There is, of course, a lawsuit in progress.  

These are _dangerous, stupid systems,_ and their deployment seemingly everywhere is going
to be a bonanza for liability lawyers.  For the companies deploying them, not so much.  

The FTC has already issued warnings.  Whether corporate management will hear them, above
the siren song of huge-but-fictitious profits, is another matter.  

That siren song was recently put sharply into perspective for me:  

<a href="https://mathstodon.xyz/@jenniferplusplus@hachyderm.io/112678572080009019"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mastodon-3.jpg" width="550" height="362" alt="'JenniferPlusPlus @ Mastodon: AI hype as negging" title="'JenniferPlusPlus @ Mastodon: AI hype as negging" style="margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>

'Negging' is a particularly odious tactic used by unscrupulous sorts to seduce sexual
partners by preying upon their insecurities (pointing out "negatives").  Similarly here,
AI marketing is negging for the 'brogrammers' who view themselves as cutting edge, don't
want to be left behind, and have no history with this kind of manipulation to help them
resist it.  

Disturbing, but it seems apt.  

### The Rising Rage Against the Machine  

People who _make_ things, who actually have to _do_ things, seem to have caught on.  Their
management, as seems to be always the case, has not:  
<a href="https://mathstodon.xyz/@mhoye@mastodon.social/112671908856314383"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mastodon-2.jpg" width="550" height="430" alt="Corporate management: the usual aggravated stupidity" title="Corporate management: the usual aggravated stupidity" style="margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>

In my employment years, I watched almost that exact scenario play out multiple times.  A
manager who is possessed of only half a clue about some new-ish method or technology tries
to force it _everywhere._  The results are uniformly disastrous, and eventually get blamed
on "Expert A" above, who was screaming "No!" all the time.  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-ludic-1.jpg" width="400" height="124" alt="Ludic: A rant against management stupidity on AI" title="Ludic: A rant against management stupidity on AI" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
A colorful case in point is a rant <sup id="fn9a">[[9]](#fn9)</sup> that got considerable
circulation in the more math/sci/tech corners of social media a couple weeks ago. Like
many of my generation, I generally avoid profanity.  However, even I admit that one must
have something in reserve for special cases, and this is indeed a&hellip; _special_ case.  

It expresses &ndash; eloquently, ribaldly, and scatologically &ndash; the absolute _rage_
people with technical and scientific training have for the latest dumb management
positions on AI.  It is also _hilarious._  (Never neglect humor: it may be your sharpest
rhetorical tool!)  

Almost all the current AI hype is from grifters, and the summary should be:  
> We have a few key things that a grifter does not have, such as job stability, genuine
> friendships, and __souls.__  
>  
> &hellip;  
>  
> This entire class of person is, to put it simply, abhorrent to right-thinking
> people. They're an embarrassment to people that are actually making advances in the
> field, a disgrace to people that know how to sensibly use technology to improve the
> world, and are also a bunch of tedious know-nothing bastards that should be thrown into
> Thought Leader Jail until they've learned their lesson&hellip;  

I spent decades in applied statistics and machine learning.  And yes, that's how we
generally see the managers yattering on subjects about which they know nothing.  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-ethics-info-tech-1.jpg" width="400" height="232" alt="Hicks et al. @ Ethics &amp; Info Tech: ChatGPT is BS" title="Hicks et al. @ Ethics &amp; Info Tech: ChatGPT is BS" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
Now, for those of you whose taste runs to somewhat less profanity and somewhat more peer
review, here is a peer-reviewed article saying largely the same 
thing. <sup id="fn10a">[[10]](#fn10)</sup>  But with less profanity.  And, alas, less
humor.  

They _did_ manage to get the word 'bullsh\*t' past the editors, so kudos for that.
And&hellip; who knew?  There turns out to be a whole academic classification scheme for
the various forms and manifestations of bovine ordure.  As they point out, St Augustine
distinguished 7 types of lies that go against truth, so it should not be surprising to
find various ways of _ignoring_ the truth.  

But their conclusion is simply, ChatGPT is bullsh\*t:  

> Calling their mistakes ‘hallucinations’ isn’t harmless: it lends itself to the confusion
> that the machines are in some way misperceiving but are nonetheless trying to convey
> something that they believe or have perceived. This, as we’ve argued, is the wrong
> metaphor. The machines are not trying to communicate something they believe or
> perceive. Their inaccuracy is not due to misperception or hallucination. As we have
> pointed out, they are not trying to convey information at all. They are bullsh\*tting.  
>  
> &hellip;  
>  
> &hellip; the inaccuracies show that it is bullsh\*tting, even when it’s right.  Calling
> these inaccuracies ‘bullsh\*t’ rather than ‘hallucinations’ isn’t just more accurate (as
> we’ve argued); it’s good science and technology communication in an area that sorely
> needs it.  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-bi-1.jpg" width="400" height="143" alt="Balevic @ Business Insider: Goldman Sachs warns about AI ROI" title="Balevic @ Business Insider: Goldman Sachs warns about AI ROI" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
Even the finance bros seem to have caught on: Goldman Sachs is now warning that the
returns from AI investment are&hellip; well, not what people think. <sup id="fn11a">[[11]](#fn11)</sup>

Apparently we've spent about a billion, and are on track to spend a trillion, for systems
that are persuasive, but lie like Donald Trump.  (My comparison, not theirs.  But it seems
apt, no?)  

Worst of all, we're sacrificing our power grids, our fresh water capacity, the careers
of creative freelancers to feed these monsters, and the factual integrity of our
scientific literature in return for _nothing useful:_  
<a href="https://bsky.app/profile/coreybrickley.bsky.social/post/3kvyga5yxgg2q"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mastodon-4.jpg" width="550" height="752" alt="Brickley @ Mastodon: The costs of AI stupidity" title="Brickley @ Mastodon: The costs of AI stupidity" style="margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>


## The Weekend Conclusion  

Look, it's not hard: these things are _unfit for any purpose._  

Even if your purpose is just to generate a fantasy, do you really want your fantasies
shaped by something scraped off the bottom of the Internet?  Wouldn't it be healthier to
train your own imagination?  

This whole affair is an instance of [Brandolini's Law](https://en.wikipedia.org/wiki/Brandolini%27s_law):  

> The amount of energy needed to refute bullsh*t is an order of magnitude bigger than that
> needed to produce it.  

<img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-wikipedia-1.jpg" width="100" height="197" alt="Wikipedia: parasitic worm Leucochloridium paradoxum" title="Wikipedia: parasitic worm Leucochloridium paradoxum" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
They'll keep doing it until either they win, or as John Rogers 3rd Rule of Crime
suggests above, some Rich White Guy goes to jail.  We should all hope for the latter.  

Via author Charlie Stross, who is fascinated by parasites to put in his novels, these
things are the intellectual equivalent of
[_Leucocloridium paradoxum_](https://en.wikipedia.org/wiki/Leucochloridium_paradoxum). <sup id="fn12a">[[12]](#fn12)</sup> 
That's a particularly disgusting worm that infests the brain and eye stalks of snails,
changing the snail to suicidal behavior leading to predation by birds, so the worm can
progress to its next host.  You don't want it.  Really.  

<a href="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mildly-alarmed-cats.jpg"><img src="{{ site.baseurl }}/images/2024-06-22-llm-ai-still-bs-mildly-alarmed-cats-thumb.jpg" width="400" height="300" alt="The Weekend Publisher &amp; Assistant Weekend Publisher are mildly alarmed." title="The Weekend Publisher &amp; Assistant Weekend Publisher are mildly alarmed." style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;"></a>
Here at Ch&acirc;teau Weekend, we're somewhat alarmed at this.  As you can see in the
picture, both the Weekend Publisher and the Assistant Weekend Publisher are mildly
alarmed, disturbing their natural cat aplomb.  

We're only _mildly_ alarmed, because we have boundless faith in the ability of corporate
managers to screw it up into a state of relative harmlessness.  The radioactivity inside
the blast radius may destroy their company, but probably not humanity.  

At least, until some idiot makes an ASI.  For preventing that, you need to understand
[Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky).  (It's hard for your
humble Weekend Editor to understand why so many people so stubbornly _resist_ understanding
that.)  

Of course now, more than ever: [_Ceterum censeo, Trump incarcerandam esse._]({{ site.baseurl }}/trump-danger-test/#the-weekend-conclusion)  

---

## Notes &amp; References  

<!--
<sup id="fn1a">[[1]](#fn1)</sup>

<a id="fn1">1</a>: ***, ["***"](***), *** DOI: [***](***). [↩](#fn1a)  

<a href="{{ site.baseurl }}/images/***">
  <img src="{{ site.baseurl }}/images/***" width="400" height="***" alt="***" title="***" style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;">
</a>

<a href="***">
  <img src="{{ site.baseurl }}/images/***" width="550" height="***" alt="***" title="***" style="margin: 3px 3px 3px 3px; border: 1px solid #000000;">
</a>

<iframe width="400" height="224" src="***" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="float: right; margin: 3px 3px 3px 3px; border: 1px solid #000000;"></iframe>
-->

<a id="fn1">1</a>: [Weekend Editor](mailto:SomeWeekendReadingEditor@gmail.com), ["On ChatGPT and Its Ilk"]({{ site.baseurl }}/on-chatgpt/), _Some Weekend Reading_ blog, 2023-Feb-15. [↩](#fn1a)  

<a id="fn2">2</a>: [Weekend Editor](mailto:SomeWeekendReadingEditor@gmail.com), ["ChatGPT and Francophone Misbranding"]({{ site.baseurl }}/misbranding-chatgpt-french/), _Some Weekend Reading_ blog, 2023-Mar-25. [↩](#fn2a)  

<a id="fn3">3</a>: [Weekend Editor](mailto:SomeWeekendReadingEditor@gmail.com), ["On Using ChatGPT for Peer Review"]({{ site.baseurl }}/chatgpt-vs-peer-review/), _Some Weekend Reading_ blog, 2023-Jun-14. [↩](#fn3a)  

<a id="fn4">4</a>: [Weekend Editor](mailto:SomeWeekendReadingEditor@gmail.com), ["On Detecting Academic Papers with AI-Written Content"]({{ site.baseurl }}/detecting-ai-written-papers/), _Some Weekend Reading_ blog, 2024-Mar-27. [↩](#fn4a)  

<a id="fn5">5</a>: V Tangerman, ["Tim Cook Admits Apple May Never Be Able to Make Its AI Stop Lying"](https://futurism.com/the-byte/tim-cook-admits-apple-ai-stop-lying), _Futurism: The Byte_, 2024-Jun-13. __NB:__ This includes some quotes [from a _WaPo_ article](https://www.washingtonpost.com/opinions/2024/06/11/tim-cook-apple-interview/), but it's paywalled and the [Wayback Machine](https://web.archive.org/) won't cough it up, for no particularly obvious reason. [↩](#fn5a)  

<a id="fn6">6</a>: DP Williams, ["Bias Optimizers"](https://www.americanscientist.org/article/bias-optimizers), _American Scientist_ 111:4, p. 204, 2023-Jul/Aug. DOI: [10.1511/2023.111.4.204](https://doi.org/10.1511/2023.111.4.204). Alas, sadly paywalled. [↩](#fn6a)  

<a id="fn7">7</a>: M Wong, ["Google Is Turning Into a Libel Machine"](https://www.theatlantic.com/technology/archive/2024/06/google-ai-overview-libel/678751/), _The Atlantic_, 2024-Jun-21.  A [non-paywalled version at the Wayback Machine](https://web.archive.org/web/20240622030058/https://www.theatlantic.com/technology/archive/2024/06/google-ai-overview-libel/678751/) is available. [↩](#fn7a)  

<a id="fn8">8</a>: T Hunter, ["Using AI to spot edible mushrooms could kill you"](https://www.washingtonpost.com/technology/2024/03/18/ai-mushroom-id-accuracy/), _Washington Post_, 2024-Mar-18. A [non-paywalled version at the Wayback Machine](https://web.archive.org/web/20240330174800/https://www.washingtonpost.com/technology/2024/03/18/ai-mushroom-id-accuracy/) is available. [↩](#fn8a)  

<a id="fn9">9</a>: Nik Suresh (a.k.a. "Ludic"), ["I Will F\*\*\*ing Piledrive You If You Mention AI Again"](https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/), _Lucidity_ blog, 2024-Jun-19. [↩](#fn9a)  

<a id="fn10">10</a>: MT Hicks, J Humphries, and J Slater, ["ChatGPT is bullsh\*t"](https://link.springer.com/article/10.1007/s10676-024-09775-5), _Ethics &amp; Info Tech_ 26:38, 2024-Jun-08.  DOI: [10.1007/s10676-024-09775-5](https://doi.org/10.1007/s10676-024-09775-5). [↩](#fn10a)  

<a id="fn11">11</a>: K Balevic, ["Goldman Sachs says the return on investment for AI might be disappointing"](https://www-businessinsider-com.cdn.ampproject.org/c/s/www.businessinsider.com/ai-return-investment-disappointing-goldman-sachs-report-2024-6), _Business Insider_, 2024-Jun-29. [↩](#fn11a)  

<a id="fn12">12</a>: Yes, this means I've just compared LLM AIs to a parasitic worm; that's as intended.  Yes, it also means I've compared humans to snails; that's not as intended, it's just that every analogy has its breaking point. 

Also: yes, I also know the difference between _metonym_ and _synedoche_; no, I will not get
twisted into knots over it. [↩](#fn12a)
